# @package _global_

# to execute this experiment run:
# python train.py experiment=pdb_prot_na_gen_se3

defaults:
  - override /data: pdb_na.yaml
  - override /model: pdb_prot_na_gen_se3_module.yaml
  - override /callbacks: default.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

tags: ["pdb", "prot_na_gen", "se3"]

seed: 12345

trainer:
  min_epochs: 100
  max_epochs: 1000

callbacks:
  model_checkpoint:
    monitor: "val/na_num_c4_prime_steric_clashes"
    mode: "min"

model:
  optimizer:
    lr: 0.0001

data:
  data_cfg:
    batch_size: 256

logger:
  # mlflow:
  #   _target_: lightning.pytorch.loggers.mlflow.MLFlowLogger
  #   experiment_name: MMDiff
  #   run_name: ${now:%Y-%m-%d}_${now:%H-%M-%S}_PDBProtGenSE3
  #   # tags: ${tags}
  #   # save_dir: "./mlruns"
  #   # log_model: false
  #   prefix: ""
  #   artifact_location: null
  #   # run_id: ""
  wandb:
    _target_: lightning.pytorch.loggers.wandb.WandbLogger
    # name: "" # name of the run (normally generated by wandb)
    save_dir: "${paths.output_dir}"
    offline: False
    id: null # pass correct id to resume experiment!
    anonymous: null # enable anonymous logging
    project: "MMDiff"
    log_model: False # upload lightning ckpts
    prefix: "" # a string to put at the beginning of metric keys
    # entity: "" # set to name of your wandb team
    group: ""
    tags: []
    job_type: ""
